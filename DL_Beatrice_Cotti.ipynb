{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlpWgRSQVcuv"
      },
      "source": [
        "# Deep Learning Project - A.A. 2022/2023\n",
        "\n",
        "Students:\n",
        "- Matteo Beatrice\n",
        "- Luca Cotti\n",
        "\n",
        "In this notebook we develop a CycleGAN architecture to convert photos to a Monet-like style, as a submission to the [I'm Something of a Painter Myself](https://www.kaggle.com/c/gan-getting-started) Kaggle competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Z30qlkAOvv"
      },
      "source": [
        "## Initial Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itA7ImX97atS"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nliRnPHp7ggX"
      },
      "outputs": [],
      "source": [
        "HEIGHT = 256 # Height of images\n",
        "WIDTH = 256 # Width of images\n",
        "CHANNELS = 3 # Number of channels of images (RGB => 3 channles)\n",
        "HEIGHT_RESIZE = 128 # Height of image crop augmentation\n",
        "WIDTH_RESIZE = 128 # Width of image crop augmentation\n",
        "\n",
        "TRANSFORMER_BLOCKS = 6 # Number of transformer blocks in CycleGAN model\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LAMBDA_ID=1e-5\n",
        "LAMBDA=10\n",
        "GAMMA=1e-4\n",
        "LAMBDA_START = 3\n",
        "LAMBDA_END = 1e-4\n",
        "GAMMA_START = 1e-4\n",
        "GAMMA_END = 0.999\n",
        "\n",
        "USE_BETTER_CYCLES = True # Whether to use better CycleGAN cycles or not\n",
        "USE_UNET = True # Whether to use UNET CycleGAN model instead of the basic one\n",
        "USE_DIFF_AUGMENT = False # Whether to use DiffAugment\n",
        "USE_SAVED_WEIGHTS = False # Whether to use a pretrained model or not\n",
        "\n",
        "WEIGHTS_FILE_NAME = \"dl_beatrice_cotti.keras\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg8a_P15AJ1n"
      },
      "source": [
        "### Import Packages\n",
        "\n",
        "Let's begin by identifying if the notebook is running on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4vrRrLb2JjpR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "IS_COLAB = os.getenv(\"COLAB_RELEASE_TAG\") is not None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8v0_tw9Jp4C"
      },
      "source": [
        "\n",
        "The following packages need to be downloaded on Google Colab:\n",
        "- `kaggle`: to download the kaggle competition, and create a submission.\n",
        "- `tensorflow-addons`: provides `InstanceNormalization` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPqt8NvEAuU1",
        "outputId": "3464b0b9-f8eb-4245-fdd5-7723dc809a66"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    %pip install -q kaggle tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-avnn861EO4g",
        "outputId": "ba535a8b-1790-47d6-d7a1-6ab9ec39aea1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-21 21:47:26.625635: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-21 21:47:26.966561: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-21 21:47:26.968251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-21 21:47:28.675445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import random, re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_iLIyOSEWPx"
      },
      "source": [
        "### Configure Training Strategy\n",
        "\n",
        "GPU will be used for training, because TPU is not available on the uni computer.\n",
        "\n",
        "Tensorflow needs to be configured to distribute training on the GPUs that are available. On Colab at most one GPU will be available, but on the uni computer there should be two.\n",
        "\n",
        "Read the [distributed training documentation](https://www.tensorflow.org/guide/distributed_training) for more info on strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpUc3rnHAOvy",
        "outputId": "8a58a866-b394-468a-fba5-286b92839480",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs available:  0\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "Number of replicas: 1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(\"Number of GPUs available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "    # Uncomment this line to debug the device on which tensors are allocated\n",
        "    # tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "    gpus = tf.config.list_logical_devices('GPU')\n",
        "\n",
        "    # MirroredStrategy creates a copy of the model on each GPU,\n",
        "    # splitting the input data between them.\n",
        "    # If there is only one GPU, only one model will be created.\n",
        "    strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "except:\n",
        "    # Fallback if there is no GPU available\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Should be equal to the number of available gpus (or 1 if no GPU available)\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ExxeEvABV-p"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "Download the competition dataset in the `gan-getting-started` directory.\n",
        "\n",
        "The import the dataset from kaggle, you need to:\n",
        "\n",
        "1.   Create a Kaggle account\n",
        "2.   Go to your Kaggle account page\n",
        "3.   Scroll to API section\n",
        "4.   Click `Expire API Token` to remove previous tokens\n",
        "5.   Click on `Create New API Token` - It will download `kaggle.json`  file on your machine.\n",
        "6.   Upload `kaggle.json` to your Google Drive if the notebook is running on Google Colab, otherwise add it to ~/.kaggle/ on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iWeaNeoBFvt",
        "outputId": "0af50fbe-bb03-42df-8b33-0a567ec21a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chmod: cannot access '/home/luca/.kaggle/kaggle.json': No such file or directory\n",
            "zsh:1: command not found: kaggle\n",
            "rm: cannot remove 'gan-getting-started': No such file or directory\n",
            "unzip:  cannot find or open gan-getting-started.zip, gan-getting-started.zip.zip or gan-getting-started.zip.ZIP.\n",
            "rm: cannot remove 'gan-getting-started.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "\n",
        "if IS_COLAB:\n",
        "    # Mount drive dir\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Move API key file to ~/.kaggle dir\n",
        "    # TODO: change this so the key is not left in the home dir of the uni computer\n",
        "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c gan-getting-started\n",
        "!rm -r gan-getting-started # Remove dir if it already exists\n",
        "!mkdir gan-getting-started\n",
        "!unzip -qq gan-getting-started.zip -d gan-getting-started\n",
        "!rm gan-getting-started.zip\n",
        "\n",
        "DATASET_PATH = \"gan-getting-started\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-JFL54IfL7"
      },
      "source": [
        "Check the number of TFRecords and of actual image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75mXf2F0XXxH",
        "outputId": "c2216027-5493-44c9-85f5-75b5383b32a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monet TFRecord files: 0\n",
            "Monet image files: 0.0\n",
            "Photo TFRecord files: 0\n",
            "Photo image files: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Load the filenames of the TFRecords\n",
        "MONET_FILENAMES = tf.io.gfile.glob(str(DATASET_PATH + '/monet_tfrec/*.tfrec'))\n",
        "PHOTO_FILENAMES = tf.io.gfile.glob(str(DATASET_PATH + '/photo_tfrec/*.tfrec'))\n",
        "\n",
        "# Counts the amount of data items in a TFRecord\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "n_monet_samples = count_data_items(MONET_FILENAMES)\n",
        "n_photo_samples = count_data_items(PHOTO_FILENAMES)\n",
        "\n",
        "print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\n",
        "print(f'Monet image files: {n_monet_samples}')\n",
        "print(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\n",
        "print(f'Photo image files: {n_photo_samples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45zEKufhivuI"
      },
      "source": [
        "### Data Augmentations\n",
        "\n",
        "Data augmentation for GANs should be done very carefully, especially for tasks similar to style transfer. If we apply transformations that can change too much the style of the data (e.g. brightness, contrast, saturation) it can cause the generator to do not efficiently learn the base style, so in this case, we are using only spatial transformations like flips, rotations and crops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ePyw4fVIkV8R"
      },
      "outputs": [],
      "source": [
        "def data_augment(image):\n",
        "    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "\n",
        "    # Resize to 286x286\n",
        "    image = tf.image.resize(image, [286, 286])\n",
        "\n",
        "    # Random crop to HEIGHTxWIDTH\n",
        "    image = tf.image.random_crop(image, size=[HEIGHT, WIDTH, CHANNELS])\n",
        "\n",
        "    # Rotations\n",
        "    if p_rotate > .8:\n",
        "        image = tf.image.rot90(image, k=3) # rotate 270ยบ\n",
        "    elif p_rotate > .6:\n",
        "        image = tf.image.rot90(image, k=2) # rotate 180ยบ\n",
        "    elif p_rotate > .4:\n",
        "        image = tf.image.rot90(image, k=1) # rotate 90ยบ\n",
        "\n",
        "    # Flips\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    if p_spatial > .75:\n",
        "        image = tf.image.transpose(image)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IlX2HtpMEb"
      },
      "source": [
        "Differentiable Augmentation for Data-Efficient GAN Training\n",
        "Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "(https://arxiv.org/pdf/2006.10738),\n",
        "from https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_tf.py.\n",
        "\n",
        "See https://www.kaggle.com/code/unfriendlyai/diffaugment-is-all-you-need/notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "18zd2HQrnoxJ"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def DiffAugment(x, policy='', channels_first=False):\n",
        "        if policy:\n",
        "            if channels_first:\n",
        "                x = tf.transpose(x, [0, 2, 3, 1])\n",
        "            for p in policy.split(','):\n",
        "                for f in AUGMENT_FNS[p]:\n",
        "                    x = f(x)\n",
        "            if channels_first:\n",
        "                x = tf.transpose(x, [0, 3, 1, 2])\n",
        "        return x\n",
        "\n",
        "    def rand_brightness(x):\n",
        "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) - 0.5\n",
        "        x = x + magnitude\n",
        "        return x\n",
        "\n",
        "    def rand_saturation(x):\n",
        "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) * 2\n",
        "        x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
        "        x = (x - x_mean) * magnitude + x_mean\n",
        "        return x\n",
        "\n",
        "    def rand_contrast(x):\n",
        "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) + 0.5\n",
        "        x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
        "        x = (x - x_mean) * magnitude + x_mean\n",
        "        return x\n",
        "\n",
        "    def rand_translation(x, ratio=0.125):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        image_size = tf.shape(x)[1:3]\n",
        "        shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "        translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, dtype=tf.int32)\n",
        "        translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, dtype=tf.int32)\n",
        "        grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0], dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\n",
        "        grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\n",
        "        x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_x, -1), batch_dims=1)\n",
        "        x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3]), [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3])\n",
        "        return x\n",
        "\n",
        "    def rand_cutout(x, ratio=0.5):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        image_size = tf.shape(x)[1:3]\n",
        "        cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "        offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + (1 - cutout_size[0] % 2), dtype=tf.int32)\n",
        "        offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + (1 - cutout_size[1] % 2), dtype=tf.int32)\n",
        "        grid_batch, grid_x, grid_y = tf.meshgrid(tf.range(batch_size, dtype=tf.int32), tf.range(cutout_size[0], dtype=tf.int32), tf.range(cutout_size[1], dtype=tf.int32), indexing='ij')\n",
        "        cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, grid_y + offset_y - cutout_size[1] // 2], axis=-1)\n",
        "        mask_shape = tf.stack([batch_size, image_size[0], image_size[1]])\n",
        "        cutout_grid = tf.maximum(cutout_grid, 0)\n",
        "        cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, [1, 1, 1, 3]))\n",
        "        mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, cutout_size[0], cutout_size[1]], dtype=tf.float32), mask_shape), 0)\n",
        "        x = x * tf.expand_dims(mask, axis=3)\n",
        "        return x\n",
        "\n",
        "    AUGMENT_FNS = {\n",
        "        'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "        'translation': [rand_translation],\n",
        "        'cutout': [rand_cutout],\n",
        "    }\n",
        "\n",
        "    def aug(image):\n",
        "        return DiffAugment(image,\"color,translation,cutout\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjhIW0h5I3H3"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hVjbt6-bAOv1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Reads a tensor as an image.\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
        "    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n",
        "    return image\n",
        "\n",
        "# Map image values in the range [-1, 1]\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "# Reads an image from a TFRecord.\n",
        "# Because we are building a generative model,\n",
        "# we don't need the labels or the image id so we'll only return the image from the TFRecord.\n",
        "def read_tfrecord(example):\n",
        "    tfrecord_format = {\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "# Extracts the image from the TFRecord files.\n",
        "def load_dataset(filenames):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Loads the dataset and applies the selected options.\n",
        "def get_dataset(filenames, augment=None, repeat=True, shuffle=True, batch_size=1):\n",
        "    dataset = load_dataset(filenames)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(512)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7N2QUeBAOv3"
      },
      "source": [
        "Let's test if the datasets loads correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "IMdDUXTlAOv3",
        "outputId": "324afa7f-3184-47e6-d9d3-3d76099c4baa",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "StopIteration",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    815\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n",
            "File \u001b[0;32m~/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 777\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    778\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    779\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    780\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    782\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n",
            "File \u001b[0;32m~/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3029\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n",
            "File \u001b[0;32m~/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6655\u001b[0m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} End of sequence [Op:IteratorGetNext] name: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m monet_ds \u001b[39m=\u001b[39m get_dataset(MONET_FILENAMES)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m photo_ds \u001b[39m=\u001b[39m get_dataset(PHOTO_FILENAMES)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m example_monet \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(monet_ds))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m example_photo \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(photo_ds))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/DL_Beatrice_Cotti.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m121\u001b[39m)\n",
            "File \u001b[0;32m~/Uni/deep_learning/DL_Beatrice_Cotti/env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:816\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_internal()\n\u001b[1;32m    815\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m--> 816\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ],
      "source": [
        "monet_ds = get_dataset(MONET_FILENAMES)\n",
        "photo_ds = get_dataset(PHOTO_FILENAMES)\n",
        "\n",
        "example_monet = next(iter(monet_ds))\n",
        "example_photo = next(iter(photo_ds))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Photo')\n",
        "plt.imshow(example_photo[0]* 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Monet')\n",
        "plt.imshow(example_monet[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U8rbu76ZuTA"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ae9t9DV778v"
      },
      "source": [
        "### Basic Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYwzJMb08Jvy"
      },
      "outputs": [],
      "source": [
        "conv_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "def encoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, activation=layers.ReLU(), name='block_x'):\n",
        "    block = layers.Conv2D(filters, size,\n",
        "                     strides=strides,\n",
        "                     padding='same',\n",
        "                     use_bias=False,\n",
        "                     kernel_initializer=conv_initializer,\n",
        "                     name=f'encoder_{name}')(input_layer)\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "\n",
        "    block = activation(block)\n",
        "\n",
        "    return block\n",
        "\n",
        "def transformer_block(input_layer, size=3, strides=1, name='block_x'):\n",
        "    filters = input_layer.shape[-1]\n",
        "\n",
        "    block = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False,\n",
        "                     kernel_initializer=conv_initializer, name=f'transformer_{name}_1')(input_layer)\n",
        "    block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "    block = layers.ReLU()(block)\n",
        "\n",
        "    block = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False,\n",
        "                     kernel_initializer=conv_initializer, name=f'transformer_{name}_2')(block)\n",
        "    block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "\n",
        "    block = layers.Add()([block, input_layer])\n",
        "\n",
        "    return block\n",
        "\n",
        "def decoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, name='block_x'):\n",
        "    block = layers.Conv2DTranspose(filters, size,\n",
        "                              strides=strides,\n",
        "                              padding='same',\n",
        "                              use_bias=False,\n",
        "                              kernel_initializer=conv_initializer,\n",
        "                              name=f'decoder_{name}')(input_layer)\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "\n",
        "    block = layers.ReLU()(block)\n",
        "\n",
        "    return block\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True, add_noise=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if add_noise:\n",
        "        result.add(layers.GaussianNoise(0.2))\n",
        "    if apply_instancenorm:\n",
        "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A8bNhoy8Alh"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JI5sFgy6LPa"
      },
      "outputs": [],
      "source": [
        "def BasicGenerator(height=HEIGHT, width=WIDTH, channels=CHANNELS, transformer_blocks=TRANSFORMER_BLOCKS):\n",
        "    inputs = layers.Input(shape=[height, width, channels], name='input_image')\n",
        "\n",
        "    # Encoder\n",
        "    enc_1 = encoder_block(inputs, 64,  7, 1, apply_instancenorm=False, activation=layers.ReLU(), name='block_1') # (bs, 256, 256, 64)\n",
        "    enc_2 = encoder_block(enc_1, 128, 3, 2, apply_instancenorm=True, activation=layers.ReLU(), name='block_2')   # (bs, 128, 128, 128)\n",
        "    enc_3 = encoder_block(enc_2, 256, 3, 2, apply_instancenorm=True, activation=layers.ReLU(), name='block_3')   # (bs, 64, 64, 256)\n",
        "\n",
        "    # Transformer\n",
        "    x = enc_3\n",
        "    for n in range(transformer_blocks):\n",
        "        x = transformer_block(x, 3, 1, name=f'block_{n+1}') # (bs, 64, 64, 256)\n",
        "\n",
        "    # Decoder\n",
        "    x_skip = layers.Concatenate(name='enc_dec_skip_1')([x, enc_3]) # encoder - decoder skip connection\n",
        "\n",
        "    dec_1 = decoder_block(x_skip, 128, 3, 2, apply_instancenorm=True, name='block_1') # (bs, 128, 128, 128)\n",
        "    x_skip = layers.Concatenate(name='enc_dec_skip_2')([dec_1, enc_2]) # encoder - decoder skip connection\n",
        "\n",
        "    dec_2 = decoder_block(x_skip, 64,  3, 2, apply_instancenorm=True, name='block_2') # (bs, 256, 256, 64)\n",
        "    x_skip = layers.Concatenate(name='enc_dec_skip_3')([dec_2, enc_1]) # encoder - decoder skip connection\n",
        "\n",
        "    last = layers.Conv2D(CHANNELS, 7,\n",
        "                              strides=1, padding='same',\n",
        "                              kernel_initializer=conv_initializer,\n",
        "                              use_bias=False,\n",
        "                              activation='tanh',\n",
        "                              name='decoder_output_block')(x_skip) # (bs, 256, 256, 3)\n",
        "    outputs = last\n",
        "\n",
        "    generator = keras.Model(inputs, outputs)\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sppARRBkSq3"
      },
      "outputs": [],
      "source": [
        "def UNETGenerator(height=HEIGHT, width=WIDTH, channels=CHANNELS):\n",
        "    inputs = layers.Input(shape=[height,width,channels])\n",
        "\n",
        "    # bs = batch size\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4), # (bs, 32, 32, 512)\n",
        "        upsample(128, 4), # (bs, 64, 64, 256)\n",
        "        upsample(64, 4), # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(CHANNELS, 4,\n",
        "                                  strides=2,\n",
        "                                  padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlF8Ola79-aY"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYZ7MH0_9_7V"
      },
      "outputs": [],
      "source": [
        "def BasicDiscriminator(height=HEIGHT, width=WIDTH, channels=CHANNELS):\n",
        "    inputs = layers.Input(shape=[height, width, channels], name='input_image')\n",
        "\n",
        "    # Encoder\n",
        "    x = encoder_block(inputs, 64,  4, 2, apply_instancenorm=False, activation=layers.LeakyReLU(0.2), name='block_1') # (bs, 128, 128, 64)\n",
        "    x = encoder_block(x, 128, 4, 2, apply_instancenorm=True, activation=layers.LeakyReLU(0.2), name='block_2')       # (bs, 64, 64, 128)\n",
        "    x = encoder_block(x, 256, 4, 2, apply_instancenorm=True, activation=layers.LeakyReLU(0.2), name='block_3')       # (bs, 32, 32, 256)\n",
        "    x = encoder_block(x, 512, 4, 1, apply_instancenorm=True, activation=layers.LeakyReLU(0.2), name='block_4')       # (bs, 32, 32, 512)\n",
        "\n",
        "    outputs = layers.Conv2D(1, 4, strides=1, padding='valid', kernel_initializer=conv_initializer)(x)                # (bs, 29, 29, 1)\n",
        "\n",
        "    discriminator = keras.Model(inputs, outputs)\n",
        "\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Z3m2rhqNX4"
      },
      "outputs": [],
      "source": [
        "def UNETDiscriminator(height=HEIGHT, width=WIDTH, channels=CHANNELS):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[height, width, channels], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    down1 = downsample(64, 4, False, add_noise=True)(x) # (bs, 128, 128, 64)\n",
        "    down2 = downsample(128, 4, add_noise=True)(down1) # (bs, 64, 64, 128)\n",
        "    down3 = downsample(256, 4, add_noise=True)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
        "\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y36uTX4KAOv6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    if USE_UNET:\n",
        "        monet_generator = UNETGenerator() # transforms photos to Monet-esque paintings\n",
        "        photo_generator = UNETGenerator() # transforms Monet paintings to be more like photos\n",
        "        monet_discriminator = UNETDiscriminator() # differentiates real Monet paintings and generated Monet paintings\n",
        "        photo_discriminator = UNETDiscriminator() # differentiates real photos and generated photos\n",
        "    else:\n",
        "        monet_generator = BasicGenerator(transformer_blocks=TRANSFORMER_BLOCKS) # transforms photos to Monet-esque paintings\n",
        "        photo_generator = BasicGenerator(transformer_blocks=TRANSFORMER_BLOCKS) # transforms Monet paintings to be more like photos\n",
        "        monet_discriminator = BasicDiscriminator() # differentiates real Monet paintings and generated Monet paintings\n",
        "        photo_discriminator = BasicDiscriminator() # differentiates real photos and generated photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGX7v-LSAOv7"
      },
      "source": [
        "### CycleGAN\n",
        "\n",
        "We will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n",
        "\n",
        "The losses are defined in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ido4lM0WAOv8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        monet_generator,\n",
        "        photo_generator,\n",
        "        monet_discriminator,\n",
        "        photo_discriminator,\n",
        "        lambda_cycle=LAMBDA,\n",
        "        lambda_id_cycle=LAMBDA_ID,\n",
        "        gamma_cycle=GAMMA,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.m_gen = monet_generator\n",
        "        self.p_gen = photo_generator\n",
        "        self.m_disc = monet_discriminator\n",
        "        self.p_disc = photo_discriminator\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_id_cycle = lambda_id_cycle\n",
        "        self.gamma_cycle = gamma_cycle\n",
        "        self.m_disc_feat = keras.Model(inputs=self.m_disc.input,\n",
        "                                              outputs=self.m_disc.layers[-2].output)\n",
        "        self.p_disc_feat = keras.Model(inputs=self.p_disc.input,\n",
        "                                              outputs=self.p_disc.layers[-2].output)\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        m_gen_optimizer,\n",
        "        p_gen_optimizer,\n",
        "        m_disc_optimizer,\n",
        "        p_disc_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        cycle_loss_fn,\n",
        "        identity_loss_fn,\n",
        "        aug_fn\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.m_gen_optimizer = m_gen_optimizer\n",
        "        self.p_gen_optimizer = p_gen_optimizer\n",
        "        self.m_disc_optimizer = m_disc_optimizer\n",
        "        self.p_disc_optimizer = p_disc_optimizer\n",
        "        self.gen_loss_fn = gen_loss_fn\n",
        "        self.disc_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "        self.aug_fn = aug_fn\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        real_monet, real_photo = batch_data\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # photo to monet back to photo\n",
        "            fake_monet = self.m_gen(real_photo, training=True)\n",
        "            cycled_photo = self.p_gen(fake_monet, training=True)\n",
        "\n",
        "            # monet to photo back to monet\n",
        "            fake_photo = self.p_gen(real_monet, training=True)\n",
        "            cycled_monet = self.m_gen(fake_photo, training=True)\n",
        "\n",
        "            # generating itself\n",
        "            same_monet = self.m_gen(real_monet, training=True)\n",
        "            same_photo = self.p_gen(real_photo, training=True)\n",
        "\n",
        "            # DiffAugment\n",
        "            if USE_DIFF_AUGMENT:\n",
        "                both_monet = tf.concat([real_monet, fake_monet], axis=0)\n",
        "                aug_monet = self.aug_fn(both_monet)\n",
        "                aug_real_monet = aug_monet[:BATCH_SIZE]\n",
        "                aug_fake_monet = aug_monet[BATCH_SIZE:]\n",
        "\n",
        "                disc_real_monet = self.m_disc(aug_real_monet, training=True)\n",
        "                disc_fake_monet = self.m_disc(aug_fake_monet, training=True)\n",
        "            else:\n",
        "                disc_real_monet = self.m_disc(real_monet, training=True)\n",
        "                disc_fake_monet = self.m_disc(fake_monet, training=True)\n",
        "\n",
        "            disc_real_photo = self.p_disc(real_photo, training=True)\n",
        "            disc_fake_photo = self.p_disc(fake_photo, training=True)\n",
        "\n",
        "            # evaluates generator loss\n",
        "            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n",
        "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
        "\n",
        "            # evaluates total cycle consistency loss\n",
        "            if USE_BETTER_CYCLES:\n",
        "                disc_feat_photo = self.p_disc_feat(real_photo, training=True)\n",
        "                disc_feat_cycled_photo = self.p_disc_feat(cycled_photo, training=True)\n",
        "                disc_feat_monet = self.m_disc_feat(real_monet, training=True)\n",
        "                disc_feat_cycled_monet = self.m_disc_feat(cycled_monet, training=True)\n",
        "\n",
        "                photo_cycle_loss = (self.cycle_loss_fn(real_photo, cycled_photo, 1-self.gamma_cycle) +\n",
        "                                    self.cycle_loss_fn(disc_feat_photo, disc_feat_cycled_photo, self.gamma_cycle))\n",
        "                monet_cycle_loss = (self.cycle_loss_fn(real_monet, cycled_monet, 1-self.gamma_cycle) +\n",
        "                                    self.cycle_loss_fn(disc_feat_monet, disc_feat_cycled_monet, self.gamma_cycle))\n",
        "\n",
        "                total_cycle_loss = self.lambda_cycle * (photo_cycle_loss + monet_cycle_loss)\n",
        "            else:\n",
        "                total_cycle_loss = (self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) +\n",
        "                                    self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle))\n",
        "\n",
        "            # evaluates total generator loss\n",
        "            total_monet_gen_loss = (monet_gen_loss + total_cycle_loss +\n",
        "                                    self.identity_loss_fn(real_monet, same_monet, self.lambda_id_cycle))\n",
        "            total_photo_gen_loss = (photo_gen_loss + total_cycle_loss\n",
        "                                    + self.identity_loss_fn(real_photo, same_photo, self.lambda_id_cycle))\n",
        "\n",
        "            # evaluates discriminator loss\n",
        "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n",
        "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
        "\n",
        "        # Calculate the gradients for generator and discriminator\n",
        "        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n",
        "                                                  self.m_gen.trainable_variables)\n",
        "        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n",
        "                                                  self.p_gen.trainable_variables)\n",
        "\n",
        "        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n",
        "                                                      self.m_disc.trainable_variables)\n",
        "        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n",
        "                                                      self.p_disc.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the optimizer\n",
        "        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n",
        "                                                 self.m_gen.trainable_variables))\n",
        "\n",
        "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n",
        "                                                 self.p_gen.trainable_variables))\n",
        "\n",
        "        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n",
        "                                                  self.m_disc.trainable_variables))\n",
        "\n",
        "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
        "                                                  self.p_disc.trainable_variables))\n",
        "\n",
        "        return {\n",
        "            \"monet_gen_loss\": total_monet_gen_loss,\n",
        "            \"photo_gen_loss\": total_photo_gen_loss,\n",
        "            \"monet_disc_loss\": monet_disc_loss,\n",
        "            \"photo_disc_loss\": photo_disc_loss\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a057Yb7JAOv8"
      },
      "source": [
        "### Loss functions\n",
        "\n",
        "The discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR8HxWbgAOv9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def discriminator_loss(real, generated):\n",
        "        real_loss = keras.losses.BinaryCrossentropy(from_logits=True,\n",
        "                                                       reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
        "        real_loss = tf.reduce_mean(real_loss)\n",
        "\n",
        "        generated_loss = keras.losses.BinaryCrossentropy(from_logits=True,\n",
        "                                                            reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
        "        generated_loss = tf.reduce_mean(generated_loss)\n",
        "\n",
        "        total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "        return total_disc_loss * 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZFuYMVAOv9"
      },
      "source": [
        "The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpgMlQXTAOv9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def generator_loss(generated):\n",
        "        loss = keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyM_GY2bAOv9"
      },
      "source": [
        "We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI_jOtHBAOv-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
        "        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "\n",
        "        return LAMBDA * loss1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx-X-D2UAOv-"
      },
      "source": [
        "The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF1_Xcc7AOv-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def identity_loss(real_image, same_image, LAMBDA):\n",
        "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "        return LAMBDA * 0.5 * loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NqcDk3-AOv-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJODixtue2BU"
      },
      "source": [
        "### Update Loss Weights Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu9L_LlsMSle"
      },
      "outputs": [],
      "source": [
        "class UpdateLossWeightsCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, epochs=EPOCHS, lambda_start=LAMBDA_START, lambda_end=LAMBDA_END, gamma_start=GAMMA_START, gamma_end=GAMMA_END):\n",
        "        super().__init__()\n",
        "        self.epochs = epochs\n",
        "        self.lambda_start = lambda_start\n",
        "        self.lambda_end = lambda_end\n",
        "        self.gamma_start = gamma_start\n",
        "        self.gamma_end = gamma_end\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.lambda_values = np.linspace(self.lambda_start, self.lambda_end, self.epochs)\n",
        "        self.gamma_values = np.linspace(self.gamma_start, self.gamma_end, self.epochs)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.model.lambda_cycle = self.lambda_values[epoch]\n",
        "        self.model.gamma_cycle = self.gamma_values[epoch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO6X6sMIM0qg"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    update_weights_cb = UpdateLossWeightsCallback(\n",
        "        EPOCHS,\n",
        "        lambda_start=LAMBDA_START,\n",
        "        lambda_end=LAMBDA_END,\n",
        "        gamma_start=GAMMA_START,\n",
        "        gamma_end=GAMMA_END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hhkgdvmjawj"
      },
      "source": [
        "### Learning Rate Schedule\n",
        "\n",
        "The original CycleGAN implementation used a constant learning rate schedule with a linear decay, I also found that the linear decay phase seems to be good at making the model more stable at the last epochs, you can check how the generator changes in a more conservative rate by the end looking at the gif images by the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn__b-lBjiT-"
      },
      "outputs": [],
      "source": [
        "class LinearScheduleWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  \"\"\"\n",
        "  Creates a schedule with a learning rate that decreases linearly after\n",
        "  linearly increasing during a warmup period.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.lr_start = 2e-4\n",
        "    self.lr_max = 2e-4\n",
        "    self.lr_min = 0.\n",
        "    self.steps_per_epoch = int(max(n_monet_samples, n_photo_samples)//BATCH_SIZE)\n",
        "    self.warmup_steps = 10\n",
        "    self.total_steps = EPOCHS * self.steps_per_epoch\n",
        "    self.hold_max_steps = self.total_steps * 0.8\n",
        "\n",
        "  @tf.function # Decorator needed to convert python if/else/boolean conditions to tensor ops\n",
        "  def __call__(self, step):\n",
        "    # step is an int tensor, which needs to be converted to float\n",
        "    step = tf.cast(step, tf.float32)\n",
        "\n",
        "    if step < self.warmup_steps:\n",
        "      return (self.lr_max - self.lr_start) / self.warmup_steps * step + self.lr_start\n",
        "    elif step < self.warmup_steps + self.hold_max_steps:\n",
        "      return self.lr_max\n",
        "    else:\n",
        "      lr = self.lr_max * ((self.total_steps - step) / (self.total_steps - self.warmup_steps - self.hold_max_steps))\n",
        "      if self.lr_min is not None:\n",
        "        lr = tf.math.maximum(self.lr_min, lr)\n",
        "      return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBRn3x8qk7kO"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lgFwcKOAOv_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Create generators\n",
        "    monet_generator_optimizer = tf.optimizers.Adam(learning_rate=LinearScheduleWithWarmup(), beta_1=0.5)\n",
        "    photo_generator_optimizer = tf.optimizers.Adam(learning_rate=LinearScheduleWithWarmup(), beta_1=0.5)\n",
        "\n",
        "    # Create discriminators\n",
        "    monet_discriminator_optimizer = tf.optimizers.Adam(learning_rate=LinearScheduleWithWarmup(), beta_1=0.5)\n",
        "    photo_discriminator_optimizer = tf.optimizers.Adam(learning_rate=LinearScheduleWithWarmup(), beta_1=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20SD_urgsRd7"
      },
      "source": [
        "### Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zQ09jNDsTbv"
      },
      "outputs": [],
      "source": [
        "monet_ds = get_dataset(MONET_FILENAMES, augment=data_augment, batch_size=BATCH_SIZE)\n",
        "photo_ds = get_dataset(PHOTO_FILENAMES, augment=data_augment, batch_size=BATCH_SIZE)\n",
        "gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n",
        "\n",
        "photo_ds_eval = get_dataset(PHOTO_FILENAMES, repeat=False, shuffle=False, batch_size=1)\n",
        "monet_ds_eval = get_dataset(MONET_FILENAMES, repeat=False, shuffle=False, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q300jnglW_N"
      },
      "source": [
        "### Launch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "yzXU27q4AOv_",
        "outputId": "cfa8f6bc-c5a0-418a-f674-adeec4c64dab",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create GAN\n",
        "with strategy.scope():\n",
        "  gan_model = CycleGan(monet_generator, photo_generator,\n",
        "                          monet_discriminator, photo_discriminator)\n",
        "\n",
        "  gan_model.compile(m_gen_optimizer=monet_generator_optimizer,\n",
        "                    p_gen_optimizer=photo_generator_optimizer,\n",
        "                    m_disc_optimizer=monet_discriminator_optimizer,\n",
        "                    p_disc_optimizer=photo_discriminator_optimizer,\n",
        "                    gen_loss_fn=generator_loss,\n",
        "                    disc_loss_fn=discriminator_loss,\n",
        "                    cycle_loss_fn=calc_cycle_loss,\n",
        "                    identity_loss_fn=identity_loss,\n",
        "                    aug_fn=aug\n",
        "                    )\n",
        "\n",
        "  callbacks=[]\n",
        "\n",
        "  if USE_BETTER_CYCLES:\n",
        "    callbacks.append(update_weights_cb)\n",
        "\n",
        "if USE_SAVED_WEIGHTS:\n",
        "    gan_model.load_weights(WEIGHTS_FILE_NAME)\n",
        "else:\n",
        "    gan_model.fit(gan_ds,\n",
        "                  epochs=EPOCHS,\n",
        "                  steps_per_epoch=(max(n_monet_samples, n_photo_samples)//BATCH_SIZE),\n",
        "                  callbacks=callbacks\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ4M5i1TAOwA"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWN3lusJNIV1"
      },
      "source": [
        "### Display Converted Photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akQPYakgI9Pi"
      },
      "outputs": [],
      "source": [
        "def display_generated_samples(ds, model, n_samples):\n",
        "    ds_iter = iter(ds)\n",
        "    for n_sample in range(n_samples):\n",
        "        example_sample = next(ds_iter)\n",
        "        generated_sample = model.predict(example_sample)\n",
        "\n",
        "        f = plt.figure(figsize=(12, 12))\n",
        "\n",
        "        plt.subplot(121)\n",
        "        plt.title('Input image')\n",
        "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.title('Generated image')\n",
        "        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "display_generated_samples(photo_ds_eval.take(8), monet_generator, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQI-ZdcNN9j"
      },
      "source": [
        "### Save Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYGo_NtqNP2p"
      },
      "outputs": [],
      "source": [
        "gan_model.save_weights(WEIGHTS_FILE_NAME)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download(WEIGHTS_FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import PIL\n",
        "\n",
        "%mkdir ./submissions\n",
        "\n",
        "i = 1\n",
        "for img in photo_ds:\n",
        "    prediction = monet_generator(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    im = PIL.Image.fromarray(prediction)\n",
        "    im.save(\"./submissions/\" + str(i) + \".jpg\")\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/luca/Uni/deep_learning/DL_Beatrice_Cotti/submission.zip'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"./submissions\", 'zip', \"./submissions\")\n",
        "\n",
        "%rm -r ./submission"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
